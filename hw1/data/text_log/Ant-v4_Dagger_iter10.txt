Loading expert policy from... aai4160/policies/experts/Ant.pkl
obs (1, 111) (1, 111)
Done restoring expert policy...
########################
logging outputs to  /root/Reinforcement-Learning-Berkely-Yonsei/aai4160/scripts/../../data/q2_Ant-v4_Dagger_iter10_Ant-v4_23-03-2024_18-22-06
########################
Using GPU id 0


********** Iteration 0 ************

Collecting expert(aai4160/expert_data/expert_data_Ant-v4.pkl) data to be used for training...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...

Saving rollouts as videos...
Eval_AverageReturn : 1111.2847900390625
Eval_StdReturn : 441.8299560546875
Eval_MaxReturn : 1538.896240234375
Eval_MinReturn : 502.9135437011719
Eval_AverageEpLen : 491.6666666666667
Train_AverageReturn : 4713.6533203125
Train_StdReturn : 12.196533203125
Train_MaxReturn : 4725.849609375
Train_MinReturn : 4701.45654296875
Train_AverageEpLen : 1000.0
Train_EnvstepsSoFar : 0
TimeSinceStart : 5.509025812149048
Training Loss : 0.16568507254123688
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 1 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...

Saving rollouts as videos...
Eval_AverageReturn : 4177.23876953125
Eval_StdReturn : 0.0
Eval_MaxReturn : 4177.23876953125
Eval_MinReturn : 4177.23876953125
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 1350.594970703125
Train_StdReturn : 804.0927124023438
Train_MaxReturn : 2919.4013671875
Train_MinReturn : 7.241912841796875
Train_AverageEpLen : 642.314
Train_EnvstepsSoFar : 642314
TimeSinceStart : 810.5178000926971
Training Loss : 0.0950528010725975
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 2 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...

Saving rollouts as videos...
Eval_AverageReturn : 4618.0029296875
Eval_StdReturn : 0.0
Eval_MaxReturn : 4618.0029296875
Eval_MinReturn : 4618.0029296875
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4078.3291015625
Train_StdReturn : 762.2098388671875
Train_MaxReturn : 4644.72314453125
Train_MinReturn : 78.3355941772461
Train_AverageEpLen : 955.979
Train_EnvstepsSoFar : 1598293
TimeSinceStart : 1994.1080956459045
Training Loss : 0.057643260806798935
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 3 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...

Saving rollouts as videos...
Eval_AverageReturn : 4729.146484375
Eval_StdReturn : 0.0
Eval_MaxReturn : 4729.146484375
Eval_MinReturn : 4729.146484375
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4599.197265625
Train_StdReturn : 336.4880676269531
Train_MaxReturn : 4943.36328125
Train_MinReturn : 547.3733520507812
Train_AverageEpLen : 993.848
Train_EnvstepsSoFar : 2592141
TimeSinceStart : 3217.9694221019745
Training Loss : 0.040117330849170685
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 4 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...

Saving rollouts as videos...
Eval_AverageReturn : 4745.6064453125
Eval_StdReturn : 0.0
Eval_MaxReturn : 4745.6064453125
Eval_MinReturn : 4745.6064453125
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4713.67529296875
Train_StdReturn : 206.64694213867188
Train_MaxReturn : 4990.35400390625
Train_MinReturn : 1504.303466796875
Train_AverageEpLen : 998.702
Train_EnvstepsSoFar : 3590843
TimeSinceStart : 4448.367160558701
Training Loss : 0.0292951837182045
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 5 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...

Saving rollouts as videos...
Eval_AverageReturn : 4885.5419921875
Eval_StdReturn : 0.0
Eval_MaxReturn : 4885.5419921875
Eval_MinReturn : 4885.5419921875
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4718.833984375
Train_StdReturn : 321.1300048828125
Train_MaxReturn : 5064.21826171875
Train_MinReturn : 337.39776611328125
Train_AverageEpLen : 993.661
Train_EnvstepsSoFar : 4584504
TimeSinceStart : 5672.758519649506
Training Loss : 0.027013815939426422
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 6 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...

Saving rollouts as videos...
Eval_AverageReturn : 4778.904296875
Eval_StdReturn : 0.0
Eval_MaxReturn : 4778.904296875
Eval_MinReturn : 4778.904296875
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4709.11376953125
Train_StdReturn : 257.0675964355469
Train_MaxReturn : 5047.0595703125
Train_MinReturn : 511.03448486328125
Train_AverageEpLen : 996.075
Train_EnvstepsSoFar : 5580579
TimeSinceStart : 6904.488893985748
Training Loss : 0.023677948862314224
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 7 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...

Saving rollouts as videos...
Eval_AverageReturn : 4738.7734375
Eval_StdReturn : 0.0
Eval_MaxReturn : 4738.7734375
Eval_MinReturn : 4738.7734375
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4758.4384765625
Train_StdReturn : 223.48365783691406
Train_MaxReturn : 5102.91015625
Train_MinReturn : 1104.280029296875
Train_AverageEpLen : 998.11
Train_EnvstepsSoFar : 6578689
TimeSinceStart : 8137.5135152339935
Training Loss : 0.023732876405119896
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 8 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...

Saving rollouts as videos...
Eval_AverageReturn : 4763.96484375
Eval_StdReturn : 0.0
Eval_MaxReturn : 4763.96484375
Eval_MinReturn : 4763.96484375
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4708.83154296875
Train_StdReturn : 310.70513916015625
Train_MaxReturn : 5044.8056640625
Train_MinReturn : 67.91061401367188
Train_AverageEpLen : 995.007
Train_EnvstepsSoFar : 7573696
TimeSinceStart : 9369.877051591873
Training Loss : 0.020376987755298615
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...




********** Iteration 9 ************

Collecting data to be used for training...

Relabelling collected observations with labels from an expert policy...

Training agent using sampled data from replay buffer...

Beginning logging procedure...

Collecting data for eval...

Saving rollouts as videos...
Eval_AverageReturn : 4804.03466796875
Eval_StdReturn : 0.0
Eval_MaxReturn : 4804.03466796875
Eval_MinReturn : 4804.03466796875
Eval_AverageEpLen : 1000.0
Train_AverageReturn : 4651.166015625
Train_StdReturn : 418.2187194824219
Train_MaxReturn : 4963.0673828125
Train_MinReturn : 51.48118591308594
Train_AverageEpLen : 990.561
Train_EnvstepsSoFar : 8564257
/root/miniconda3/envs/rl/lib/python3.8/site-packages/gym/core.py:317: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  deprecation(
/root/miniconda3/envs/rl/lib/python3.8/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.[0m
  deprecation(
/root/miniconda3/envs/rl/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)
  if not isinstance(terminated, (bool, np.bool8)):
TimeSinceStart : 10594.8926384449
Training Loss : 0.022181306034326553
Initial_DataCollection_AverageReturn : 4713.6533203125
Done logging...


